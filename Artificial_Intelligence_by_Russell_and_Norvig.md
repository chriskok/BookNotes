## Artificial Intelligence - A Modern Approach

by Stuart Russell and Peter Norvig

### Chapter 1: Introduction
* pg 2: A computer passes the Turing Test if a human interrogator posses some written questions that are answered by the computer that are indistinguishable from human writing. The computer would have to have many skills like natural language processing and automated reasoning. Created by the award winning Alan Turing (1950).
* pg 8: Fun fact - The word algorithm (and the idea of studying them) comes from al-Khowarazmi, a Persian mathematician of the 9th century, whose writings also introduced Arabic numerals and algebra to Europe. 
* pg 16: AI is founded upon the questions of many fields such as Psychology, Philosophy, Economics, Linguistics, and Cybernetics. I'll keep these questions in mind when finding out what new problems I can attempt to solve, seems pretty hard to find some right now.

### Chapter 2: Intelligent Agents
* pg 34: An agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators. Or PEAS (Performance, Environemtn, Actuators, Sensors). 
* pg 37: As a general rule, it is better to design performance measures (evaluation of environmental states) according to what one actually wants in the environment, rather than according to how one thinks the agent should behave. Like giving the agent rewards. 
* pg 37: Rationality of an agent depends on 1. The performance measure that defines the success criterion, 2. The agent's prior knowledge of the environment, 3. The actions that can be performed, 4. The agent's percept sequence to date. 
* pg 42: Fully vs Partially observable environments (the ability to measure success within the environment).
* pg 43: Single (crossword) vs Multiagent(chess) environments; where agent A might depends on another (or multiple other) agents B, C, etc. 
* pg 43: Deterministic vs Stochastic situations; most real-life ones are hard to be deterministic especially the partially oberservable. 
* pg 43: Episodic (the next episode doesn't depend on the previous ones, like american tv series) vs Sequential (current decision could affect all else, like anime).
* pg 57: Representations of agent's environments include Atomic (like a black box), Factored (like a weighted black box) and Structured (where they might all rely on other agent's).

### Chapter 3: Solving Problems By Searching
* pg 65: In general, an agent with several immediate options of unknown value can decide what to do by first examining future actions that eventually lead to states of known value.
* pg 66: "A search algorithm takes a problem as input and returns a solution in the form of an action sequence." Not what I initially thought of when I read search algorithm. Edit: now that I read further on, it's actually pretty much the same thing.
* pg 79: The typical structure of a search algorithm is;
function CHILD-NODE(problem, parent , action) returns a node
  return a node with
    STATE = problem.RESULT(parent.STATE, action),
    PARENT = parent, ACTION = action,
    PATH-COST = parent.PATH-COST + problem.STEP-COST(parent.STATE, action)
Where: 
• n.STATE: the state in the state space to which the node corresponds;
• n.PARENT: the node in the search tree that generated this node;
• n.ACTION: the action that was applied to the parent to generate the node;
• n.PATH-COST: the cost, traditionally denoted by g(n), of the path from the initial state to the node, as indicated by the parent pointers.
* pg 84: Uniform-cost search, similar to BFS but down with a priority queue with ordering based on g(n) or the cost of each path. The minimum cost path gets expanded first and the smallest cost to get to any particular node is saved in that node.
* pg 87: Depth-limited search, similar to DFS but stops at a certain limit.
* pg 89: Iterative deepening DFS, using DFS but runs through it at each level until it finds all goals (we say it runs till the depth of the shallowest goal node, d). O(b^d) time, O(bd) space. 
* pg 91: Bidirectional search, run two searches, one from the goal and one from the initial state and hope they meet. The idea is that b^(d/2) + b^(d/2) is much less than b^d. O(b^(d/2)) time and space.
* pg 93: A* search, form of best-first search where f(n) = g(n) + h(n) or cost to reach the node plus cost to get from the node to the goal. 
* pg 94: For this to be correct, h(n) must be an admissible heuristic that doesn't overestimate. 
* pg 95: Also, it has to be consistent- A heuristic h(n) is consistent if, for every node n and every successor n' of n generated by any action a, the estimated cost of reaching the goal from n is no greater than the step cost of getting to n' plus the estimated cost of reaching the goal from n'.
* pg 98: The complexity of A* often makes it impractical to insist on finding an optimal solution.
* pg 99: Iterative-deepening A* , The main difference between IDA* and standard iterative deepening is that the cutoff used is the f-cost (g+h) rather than the depth; at each iteration, the cutoff value is the smallest f-cost of any node that exceeded the cutoff on the previous iteration. 
* pg 103: Choosing the right heuristic obviously effects the performance, to measure this we can check the quality of it with the effective branching factor b* . 
* pg 103: If the total number of nodes generated by A∗ for a particular problem is N and the solution depth is d, then b* is the branching factor that a uniform tree of depth d would have to have in order to contain N + 1 nodes. In the example, they tested a bunch of examples with A* and got the b* from A* (h1) or A* (h2) and found that the method of h2 always beats h1.
* pg 105-107: We can generate these heuristics through relaxed problems (changing the stucture of the problem so as to reduce restrictions), subproblems (pattern databases where we can remove certain subproblems by finding out what is independent) and experience (solving lots of examples).






